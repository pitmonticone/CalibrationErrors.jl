var documenterSearchIndex = {"docs":
[{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"EditURL = \"https://github.com/devmotion/CalibrationErrors.jl/blob/master/examples/distribution.jl\"","category":"page"},{"location":"generated/distribution/#Distribution-of-calibration-error-estimates-1","page":"Distribution","title":"Distribution of calibration error estimates","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"(Image: ) (Image: )","category":"page"},{"location":"generated/distribution/#Introduction-1","page":"Distribution","title":"Introduction","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"This example is taken from the publication \"Calibration tests in multi-class classification: A unifying framework\" by Widmann, Lindsten, and Zachariah.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"We estimate calibration errors of the model","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"beginaligned\n   g(X) sim textrmDir(alpha)\n   Z sim textrmBer(pi)\n   Y  g(X) = gamma Z = 1 sim textrmCategorical(beta)\n   Y  g(X) = gamma Z = 0 sim textrmCategorical(gamma)\nendaligned","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"where alpha in mathbbR_0^m determines the distribution of predictions g(X), pi  0 determines the degree of miscalibration, and beta defines a fixed categorical distribution.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"Here we consider only the choices alpha = (01 ldots 01), mimicking a distribution after training that is pushed towards the edges of the probability simplex, and beta = (1 0 ldots 0).","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"In our experiments we sample 250 predictions from the Dirichlet distribution textrmDir(alpha), and then we generate corresponding labels according to the model stated above, for different choices of pi and number of classes m.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"We evaluate the standard estimators of expected calibration error (ECE) based on a uniform binning scheme and a data-dependent binning scheme, and the biased estimator of the squared kernel calibration error (SKCE), the quadratic unbiased estimator of the SKCE, and the linear unbiased estimator of the SKCE for a specific choice of matrix-valued kernels.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"The sampling procedure and the evaluation are repeated 100 times, to obtain a sample of 100 estimates for each considered setting of pi and m.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"For our choice of alpha and beta, the analytical ECE with respect to the total variation distance _mathrmTV is","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"mathbbE_mathrmTVg = fracpi(m-1)m","category":"page"},{"location":"generated/distribution/#Setup-1","page":"Distribution","title":"Setup","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"using CalibrationErrors\nusing Distances\nusing Distributions\nusing StatsBase\n\nusing LinearAlgebra\nusing Random\n\nusing Plots\ngr(fmt = :png, dpi = 600)","category":"page"},{"location":"generated/distribution/#Estimates-1","page":"Distribution","title":"Estimates","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"function estimates(rng::AbstractRNG, estimator, π::Real, m::Int)\n    # check arguments\n    m > 0 || throw(ArgumentError(\"number of classes must be positive\"))\n    zero(π) <= π <= one(π) ||\n        throw(ArgumentError(\"probability π must be between 0 and 1\"))\n\n    # cache array for predictions, modified predictions, and labels\n    predictions = Matrix{Float64}(undef, m, 250)\n    labels = Vector{Int}(undef, 250)\n    data = (predictions, labels)\n\n    # define sampler of predictions\n    sampler_predictions = sampler(Dirichlet(m, 0.1))\n\n    # initialize estimates\n    estimates = Vector{Float64}(undef, 100)\n\n    # for each run\n    @inbounds for i in eachindex(estimates)\n        # sample predictions\n        rand!(rng, sampler_predictions, predictions)\n\n        # sample labels\n        @inbounds for j in eachindex(labels)\n            if rand(rng) < π\n                labels[j] = 1\n            else\n                labels[j] = sample(rng, Weights(view(predictions, :, j), 1))\n            end\n        end\n\n        # evaluate estimator\n        estimates[i] = estimator isa CalibrationErrors.CalibrationErrorEstimator ?\n            calibrationerror(estimator, data) : calibrationerror(estimator(data), data)\n    end\n\n    estimates\nend","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"We use a helper function to run the experiment for all desired parameter settings.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"struct EstimatesSet{E}\n    estimator::E\n    m::Vector{Int}\n    π::Vector{Float64}\n    estimates::Vector{Vector{Float64}}\nend\n\nfunction estimates(rng::AbstractRNG, estimator)\n    # create arrays\n    mvec = Vector{Int}(undef, 0)\n    πvec = Vector{Float64}(undef, 0)\n    estimatesvec = Vector{Vector{Float64}}(undef, 0)\n\n    # for all combinations of m and π\n    for m in (2, 10, 100), π in (0.0, 0.5, 1.0)\n        # compute estimates\n        push!(estimatesvec, estimates(Random.GLOBAL_RNG, estimator, π, m))\n\n        # save m and π\n        push!(mvec, m)\n        push!(πvec, π)\n    end\n\n    EstimatesSet(estimator, mvec, πvec, estimatesvec)\nend\n\nestimates(estimator) = estimates(Random.GLOBAL_RNG, estimator)","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"As mentioned above, we can calculate the analytic expected calibration error. For the squared kernel calibration error, we take the mean of the estimates of the unbiased quadratic estimator as approximation of the true value.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"We provide simple histogram plots of our results. The mean value of the estimates is indicated by a solid black vertical line and the analytic calibration error is visualized as a dashed red line.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"@recipe function f(set::EstimatesSet)\n    # default settings\n    layout := (3, 3)\n    legend := false\n    xlabel := \"calibration estimate\"\n    ylabel := \"# runs\"\n    size --> (1080, 960)\n    seriestype := :histogram\n\n    # add subplots\n    for i in 1:length(set.estimates)\n        # retrieve data\n        m = set.m[i]\n        π = set.π[i]\n        estimates = set.estimates[i]\n\n        # plot histogram of estimates\n        @series begin\n            subplot := i\n            x := estimates\n        end\n\n        # indicate analytic calibration error for ECE\n        if set.estimator isa ECE\n            @series begin\n                seriestype := :vline\n                subplot := i\n                linewidth := 2\n                color := :red\n                x := [π * (m - 1) / m]\n            end\n        end\n\n        # indicate mean of estimates\n        @series begin\n            title := \"$m classes, pi = $π\"\n            seriestype := :vline\n            subplot := i\n            linewidth := 2\n            color := :black\n            x := [mean(estimates)]\n        end\n    end\nend","category":"page"},{"location":"generated/distribution/#Kernel-choice-1","page":"Distribution","title":"Kernel choice","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"We use an identity matrix scaled with an exponential kernel as matrix-valued kernel. The bandwidth of the exponential kernel is set according to the median heuristic.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"function kernel(data)\n    # compute median TV distance\n    predictions = data[1]\n    γ = inv(median(pairwise(TotalVariation(), predictions, dims=2)))\n\n    # create kernel\n    UniformScalingKernel(ExponentialKernel(γ, TotalVariation()))\nend","category":"page"},{"location":"generated/distribution/#Expected-calibration-error-1","page":"Distribution","title":"Expected calibration error","text":"","category":"section"},{"location":"generated/distribution/#Uniform-binning-1","page":"Distribution","title":"Uniform binning","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"We start by analyzing the expected calibration error (ECE). For our estimation we use 10 bins of uniform width in each dimension.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"Random.seed!(1234)\ndata = estimates(ECE(UniformBinning(10), TotalVariation()))\nplot(data)","category":"page"},{"location":"generated/distribution/#Non-uniform-binning-1","page":"Distribution","title":"Non-uniform binning","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"We repeat our experiments with a different data-dependent binning scheme. This time the bins will be computed dynamically by splitting the predictions at the median of the classes with the highest variance, as long as the number of bins does not exceed a given threshold and the number of samples per bin is above a certain lower bound. In our experiments we do not impose any restriction on the number of bins but only stop splitting if the number of samples is less than 10.","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"Random.seed!(1234)\ndata = estimates(ECE(MedianVarianceBinning(10), TotalVariation()))\nplot(data)","category":"page"},{"location":"generated/distribution/#Biased-estimator-of-the-squared-kernel-calibration-error-1","page":"Distribution","title":"Biased estimator of the squared kernel calibration error","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"Random.seed!(1234)\ndata = estimates(x -> BiasedSKCE(kernel(x)))\nplot(data)","category":"page"},{"location":"generated/distribution/#Unbiased-quadratic-estimator-of-the-squared-kernel-calibration-error-1","page":"Distribution","title":"Unbiased quadratic estimator of the squared kernel calibration error","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"Random.seed!(1234)\ndata = estimates(x -> QuadraticUnbiasedSKCE(kernel(x)))\nplot(data)","category":"page"},{"location":"generated/distribution/#Unbiased-linear-estimator-of-the-squared-kernel-calibration-error-1","page":"Distribution","title":"Unbiased linear estimator of the squared kernel calibration error","text":"","category":"section"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"Random.seed!(1234)\ndata = estimates(x -> LinearUnbiasedSKCE(kernel(x)))\nplot(data)","category":"page"},{"location":"generated/distribution/#","page":"Distribution","title":"Distribution","text":"This page was generated using Literate.jl.","category":"page"},{"location":"background/#Probabilistic-setting-1","page":"Background","title":"Probabilistic setting","text":"","category":"section"},{"location":"background/#Probabilistic-model-1","page":"Background","title":"Probabilistic model","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"A probabilistic model predicts a probability distribution of possible outputs for a given input.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"A very simple probabilistic model is a model that predicts a uniform distribution for a dice roll; there is no input and the possible outputs are the numbers 123456. A probably more complicated probabilistic model would be a model that predicts the distribution of stock price changes from the stock prices of last week; here the input are the stock prices of last week and the possible outputs are formed by the set of possible stock price changes.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"If we assume that for each input there exists an underlying unknown probability distribution that captures the relation between the given input and the possible outputs, we can aim for a model whose predicted distributions are, in some sense, close to these unknown probability distributions.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"More mathematically, if we view inputs and outputs as random variables X and Y on some probability space, we can strive for a model g that satisfies","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"    g(X) approx mu_YX(cdotX)","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"where mu_YX is a version of the conditional distribution of Y given X.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"Closeness of g(X) and mu_YX(cdotX) can be measured with distance measures of probability distributions. Two prominent families of distances are phi-divergences and integral probability metrics.","category":"page"},{"location":"background/#Classification-model-1","page":"Background","title":"Classification model","text":"","category":"section"},{"location":"background/#","page":"Background","title":"Background","text":"Here we restrict ourselves to classification models, i.e., models for which output Y takes only values from a finite set.","category":"page"},{"location":"background/#","page":"Background","title":"Background","text":"The dice roll model above is a classification model, whereas the model that predicts stock price changes is not.","category":"page"},{"location":"calibration/#Calibration-1","page":"Calibration","title":"Calibration","text":"","category":"section"},{"location":"calibration/#Motivation-1","page":"Calibration","title":"Motivation","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Ideally one would like to have a model that predicts the underlying probability distribution for almost every input, i.e., a model g such that almost always","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    g(X) = mu_YX(cdotX)","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Unfortunately, if we try to infer the model from a finite data set of inputs and outputs that is usually not possible.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"In safety-critical applications such as medical decision-making or autonomous driving, however, important decisions are based on the predictions of a model. Since we are not able to obtain the perfect model, the model has to satisfy other properties such that it is deemed trustworthy.","category":"page"},{"location":"calibration/#Definition-1","page":"Calibration","title":"Definition","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"One such property is calibration, which is also called reliability. Loosely speaking, if we repeatedly predict the same distribution for different pairs of inputs and outputs, we would like that in the long run the empirical distribution of the observed outputs is similar to the predicted probability distribution. This property guarantees that the predicted distribution is not only an arbitrary probability distribution but actually makes sense from a frequentist point of view.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"A classic example from the literature is a weather forecaster who each morning predicts the probability that it will rain during the day. If we assume that the forecaster's predictions are observed for a long time, the forecaster is called calibrated \"if among those days for which his prediction is x, the long-run relative frequency of rain is also x\".","category":"page"},{"location":"calibration/#Common-notion-1","page":"Calibration","title":"Common notion","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Commonly (see, e.g, Guo et al. (2017)), only calibration of the most-confident predictions max_y g_y(x) of a model g is considered. According to this common notion a model is calibrated if almost always","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    mathbbPY = textrmarg  max_y g_y(X)  max_y g_y(X) = max_y g_y(X)","category":"page"},{"location":"calibration/#Strong-notion-1","page":"Calibration","title":"Strong notion","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"According to the more general definition by Bröcker (2009) and Vaicenavicius et al. (2019), a probabilistic model g is calibrated if almost always","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    mathbbPY = y  g(X) = g_y(X)","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"for all classes y.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"For classification problems with more than two classes, this definition of calibration is stronger than the more common one above. By reducing the model and applying the strong notion to the simplified model, however, this definition still allows to investigate the calibration of the model with respect to only certain aspects of interest such as the calibration of the most-confident predictions.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Thus in this Julia package and its documentation, we always refer to the strong notion of calibration.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Let y_1 ldots y_m be the possible outputs. Then we can also define calibration in a vectorized form. Equivalently to the definition above, a model g is calibrated if and only if","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    r(g(X)) - g(X) = 0","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"holds almost always, where","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    r(xi) = (mathbbPY = y_1  g(X) = xi ldots mathbbPY = y_m  g(X) = xi)","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"denotes the so-called calibration function.","category":"page"},{"location":"calibration/#Measures-1","page":"Calibration","title":"Measures","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Calibration measures allow a more fine-tuned analysis of calibration and enable comparisons of calibration of different models. Intuitively, calibration measures quantify the deviation of the left and right hand side in the definitions above.","category":"page"},{"location":"calibration/#Expected-calibration-error-(ECE)-1","page":"Calibration","title":"Expected calibration error (ECE)","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"The most common calibration measure is the so-called expected calibration error (ECE) (see, e.g., Guo et al. (2017)). Informally, it is defined as the average distance between the left and right hand side of the definition above with respect to some metric. Mathematically, the expected calibration of model g with respect to distance measure d is defined as","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    mathrmECEd g = mathbbEd(r(g(X)) g(X))","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Here d could be, e.g., the cityblock distance, the total variation distance, or the squared Euclidean distance.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"If d(p q) = 0 if and only if p = q, then the ECE of model g with respect to distance measure d is zero if and only if g is calibrated.","category":"page"},{"location":"calibration/#Calibration-error-(CE)-1","page":"Calibration","title":"Calibration error (CE)","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"More generally, Widmann et al. (2019) define the calibration error (CE) of a model g with respect to a function class mathcalF subset f colon Delta^m to mathbbR^m as","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    mathrmCEmathcalF g = sup_f in mathcalF mathbbE(r(g(X)) - g(X))^intercal f(g(X))","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"If model g is calibrated, then the CE is zero, regardless of the choice of mathcalF. However, for some function spaces (e.g., for mathcalF = 0) the CE is zero even if g is not calibrated.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"Interestingly, the ECE with respect to the cityblock distance, the total variation distance, and the squared Euclidean distance are all special cases of the CE (Widmann et al. (2019)).","category":"page"},{"location":"calibration/#Kernel-calibration-error-(KCE)-1","page":"Calibration","title":"Kernel calibration error (KCE)","text":"","category":"section"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"The kernel calibration error (KCE) is another special case of the CE, in which the unit ball of a reproducing kernel Hilbert space (RKHS) of vector-valued functions is chosen as function space mathcalF.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"A RKHS of vector-valued functions f colon Delta^m to mathbbR^m can be identified with a unique matrix-valued kernel k colon Delta^m times Delta^m to mathbbR^m times m. Then the KCE of a model g with respect to kernel k is defined as","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    mathrmKCEk g = mathrmCEmathcalF g","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"where mathcalF is the unit ball of the RKHS corresponding to kernel k.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"As Widmann et al. (2019) show, for a large class of kernels (so-called universal kernels) the KCE is zero if and only if the model g is calibrated. Moreover, the KCE can be formulated in terms of the kernel k as","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"    mathrmKCEk g = left(mathbbE(e_Y - g(X))^intercal k(g(X) g(X)) (e_Y - g(X))right)^12","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"where (XY) is an independent copy of (XY) and e_i denotes the ith unit vector.","category":"page"},{"location":"calibration/#","page":"Calibration","title":"Calibration","text":"The so-called maximum mean calibration error (MMCE), proposed by Kumar et al. (2018), can be viewed as a special case of the KCE, in which only the most-confident predictions are considered (Widmann et al. (2019)).","category":"page"},{"location":"estimators/#Calibration-error-estimators-1","page":"Estimators","title":"Calibration error estimators","text":"","category":"section"},{"location":"#CalibrationErrors.jl-1","page":"Home","title":"CalibrationErrors.jl","text":"","category":"section"},{"location":"#","page":"Home","title":"Home","text":"Estimation of calibration errors.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"A package for estimating calibration errors from predictions and labels.","category":"page"},{"location":"#","page":"Home","title":"Home","text":"","category":"page"}]
}
